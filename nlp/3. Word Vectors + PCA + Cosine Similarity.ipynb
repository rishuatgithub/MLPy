{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a google embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â load the word embeddings from the google news vectors. Load it once.\n",
    "\n",
    "#embeddings = KeyedVectors.load_word2vec_format('../../Data/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a word embeddings for the small subset that is required in here\n",
    "\n",
    "f = open('../../Data/word_vectors/capitals.txt', 'r').read()\n",
    "set_words = set(nltk.word_tokenize(f))\n",
    "select_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', \n",
    "                        'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "for w in select_words:\n",
    "    set_words.add(w)\n",
    "\n",
    "\n",
    "def get_word_embeddings(embeddings):\n",
    "    '''\n",
    "        Get the word embeddings\n",
    "    '''\n",
    "    word_embeddings = {}\n",
    "    for word in embeddings.vocab:\n",
    "        if word in set_words:\n",
    "            word_embeddings[word] = embeddings[word]\n",
    "            \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = get_word_embeddings(embeddings)\n",
    "print(len(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word embeddings from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = pickle.load(open('word_embeddings_subset.p','rb'))\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings['Spain'].size ## The size of the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict relationships between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity function is:\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}$$\n",
    "$A$ and $B$ represent the word vectors and $A_i$ or $B_i$ represent index i of that vector. & Note that if A and B are identical, you will get $cos(\\theta) = 1$.\n",
    "\n",
    "- Otherwise, if they are the total opposite, meaning, $A= -B$, then you would get $cos(\\theta) = -1$.\n",
    "- If you get $cos(\\theta) =0$, that means that they are orthogonal (or perpendicular).\n",
    "- Numbers between 0 and 1 indicate a similarity score.\n",
    "- Numbers between -1-0 indicate a dissimilarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A,B):\n",
    "    '''\n",
    "        Returns the cosine similarity between vectors A and B\n",
    "    '''\n",
    "    \n",
    "    d = np.dot(A,B)\n",
    "    norm_a = np.sqrt(np.dot(A,A))\n",
    "    norm_b = np.sqrt(np.dot(B,B))\n",
    "    \n",
    "    cos = d / (norm_a * norm_b)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "\n",
    "cosine_similarity(king,queen)  ## between 0 and 1 is similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement a function that computes the similarity between two vectors using the Euclidean distance. Euclidean distance is defined as:\n",
    "$$ \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) =\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ =\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}$$\n",
    "\n",
    "- $n$ is the number of elements in the vector\n",
    "- $A$ and $B$ are the corresponding word vectors.\n",
    "- The more similar the words, the more likely the Euclidean distance will be close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(A,B):\n",
    "    '''\n",
    "        Calculate the euclidean distance between two vectors\n",
    "    '''\n",
    "    \n",
    "    d = np.linalg.norm(A - B)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4796925"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "\n",
    "euclidean_distance(king,queen) ## somewhat similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the capital of the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings):\n",
    "    '''\n",
    "        Find the most likely country (country2) for a given set of inputs\n",
    "    '''\n",
    "    \n",
    "    city1_embed = embeddings[city1]\n",
    "    country1_embed = embeddings[country1]\n",
    "    city2_embed = embeddings[city2]\n",
    "    \n",
    "    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n",
    "    # Remember: King - Man + Woman = Queen\n",
    "    \n",
    "    vec = country1_embed - city1_embed + city2_embed\n",
    "    \n",
    "    similarity = -1\n",
    "    \n",
    "    country = ''\n",
    "    \n",
    "    group = set((city1, country1, city2))\n",
    "    \n",
    "    ## iterate through all words in the embedding\n",
    "    for word in embeddings.keys():\n",
    "        \n",
    "        if word not in group:\n",
    "            \n",
    "            word_embedd = embeddings[word]\n",
    "            \n",
    "            cos_similarity = cosine_similarity(vec, word_embedd) ## find cos similarity\n",
    "            \n",
    "            if cos_similarity > similarity:\n",
    "                \n",
    "                similarity = cos_similarity\n",
    "                \n",
    "                country = (word, similarity)\n",
    "                \n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Egypt', 0.7626821)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Russia', 0.6954342)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_country('London', 'England', 'Moscow', word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city1 country1    city2     country2\n",
       "0  Athens   Greece  Bangkok     Thailand\n",
       "1  Athens   Greece  Beijing        China\n",
       "2  Athens   Greece   Berlin      Germany\n",
       "3  Athens   Greece     Bern  Switzerland\n",
       "4  Athens   Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load a sample country data set\n",
    "\n",
    "data = pd.read_csv('../../Data/word_vectors/capitals.txt', sep=' ')\n",
    "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, embeddings):\n",
    "    '''\n",
    "        get the overall accuracy of the word embedding model\n",
    "    '''\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        city1 = row['city1']\n",
    "        country1 = row['country1']\n",
    "        city2 = row['city2']\n",
    "        country2 = row['country2']\n",
    "        \n",
    "        predict_country, predict_similarity = get_country(city1, country1, city2, embeddings)\n",
    "        \n",
    "        if predict_country == country2:\n",
    "            correct += 1\n",
    "            \n",
    "    total_data = len(data)\n",
    "    \n",
    "    accuracy = correct / total_data\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :      91.92\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Accuracy : {get_accuracy(data, word_embeddings)*100:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will explore the distance between word vectors after reducing their dimension. The technique we will employ is known as principal component analysis (PCA). As we saw, we are working in a 300-dimensional space in this case. Although from a computational perspective we were able to perform a good job, it is impossible to visualize results in such high dimensional spaces.\n",
    "\n",
    "You can think of PCA as a method that projects our vectors in a space of reduced dimension, while keeping the maximum information about the original vectors in their reduced counterparts. In this case, by maximum infomation we mean that the Euclidean distance between the original vectors and their projected siblings is minimal. Hence vectors that were originally close in the embeddings dictionary, will produce lower dimensional vectors that are still close to each other.\n",
    "\n",
    "You will see that when you map out the words, similar words will be clustered next to each other. For example, the words 'sad', 'happy', 'joyful' all describe emotion and are supposed to be near each other when plotted. The words: 'oil', 'gas', and 'petroleum' all describe natural resources. Words like 'city', 'village', 'town' could be seen as synonyms and describe a similar thing.\n",
    "\n",
    "Before plotting the words, you need to first be able to reduce each word vector with PCA into 2 dimensions and then plot it. The steps to compute PCA are as follows:\n",
    "- Mean normalize the data\n",
    "- Compute the covariance matrix of your data ($\\Sigma$).\n",
    "- Compute the eigenvectors and the eigenvalues of your covariance matrix\n",
    "- Multiply the first K eigenvectors by your normalized data. The transformation should look something as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(X, n_components=2):\n",
    "    '''\n",
    "    Compute the PCA\n",
    "        \n",
    "    Input:\n",
    "          X: of dimension (m,n) where each row corresponds to a word vector\n",
    "          n_components: Number of components you want to keep.\n",
    "    Output:\n",
    "          X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
    "    '''\n",
    "    \n",
    "    # mean center the data\n",
    "    X_demeaned = X - np.mean(X,axis=0)\n",
    "    print('X_demeaned.shape: ',X_demeaned.shape)\n",
    "    \n",
    "    # calculate the covariance matrix\n",
    "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
    "    \n",
    "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n",
    "    \n",
    "    # sort eigenvalue in increasing order (get the indices from the sort)\n",
    "    idx_sorted = np.argsort(eigen_vals)\n",
    "    \n",
    "    # reverse the order so that it's from highest to lowest.\n",
    "    idx_sorted_decreasing = idx_sorted[::-1]\n",
    "    \n",
    "    # sort the eigen values by idx_sorted_decreasing\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "    \n",
    "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
    "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n",
    "    \n",
    "    # select the first n eigenvectors (n is desired dimension\n",
    "    # of rescaled data array, or dims_rescaled_data)\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n",
    "    \n",
    "    X_reduced = np.dot(eigen_vecs_subset.transpose(),X_demeaned.transpose()).transpose()\n",
    "\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01\n",
      "  1.46755891e-01 9.23385948e-02 1.86260211e-01 3.45560727e-01\n",
      "  3.96767474e-01 5.38816734e-01]\n",
      " [4.19194514e-01 6.85219500e-01 2.04452250e-01 8.78117436e-01\n",
      "  2.73875932e-02 6.70467510e-01 4.17304802e-01 5.58689828e-01\n",
      "  1.40386939e-01 1.98101489e-01]\n",
      " [8.00744569e-01 9.68261576e-01 3.13424178e-01 6.92322616e-01\n",
      "  8.76389152e-01 8.94606664e-01 8.50442114e-02 3.90547832e-02\n",
      "  1.69830420e-01 8.78142503e-01]]\n",
      "X_demeaned.shape:  (3, 10)\n",
      "Your original matrix was (3, 10) and it became:\n",
      "[[ 0.43437323  0.49820384]\n",
      " [ 0.42077249 -0.50351448]\n",
      " [-0.85514571  0.00531064]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.rand(3, 10)\n",
    "print(X)\n",
    "X_reduced = compute_pca(X, n_components=2)\n",
    "print(\"Your original matrix was \" + str(X.shape) + \" and it became:\")\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(embeddings, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        embeddings: a word \n",
    "        fr_embeddings:\n",
    "        words: a list of words\n",
    "    Output: \n",
    "        X: a matrix where the rows are the embeddings corresponding to the rows on the list\n",
    "        \n",
    "    \"\"\"\n",
    "    m = len(words)\n",
    "    X = np.zeros((1, 300))\n",
    "    for word in words:\n",
    "        english = word\n",
    "        eng_emb = embeddings[english]\n",
    "        X = np.row_stack((X, eng_emb))\n",
    "    X = X[1:,:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 11 words each of 300 dimensions thus X.shape is: (11, 300)\n"
     ]
    }
   ],
   "source": [
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "print('You have 11 words each of 300 dimensions thus X.shape is:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_demeaned.shape:  (11, 300)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We have done the plotting for you. Just run this cell.\n",
    "result = compute_pca(X, 2)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
