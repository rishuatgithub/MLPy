{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b478f53",
   "metadata": {},
   "source": [
    "Reference: https://huggingface.co/course/chapter1/4?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78a12d",
   "metadata": {},
   "source": [
    "![transformers](images/model_parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd40df",
   "metadata": {},
   "source": [
    "### Transformers are language models\n",
    "\n",
    "All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as language models. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!\n",
    "\n",
    "This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
    "\n",
    "An example of a task is predicting the next word in a sentence having read the n previous words. This is called causal language modeling because the output depends on the past and present inputs, but not the future ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855b890",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41b43c",
   "metadata": {},
   "source": [
    "![transformers](images/pretraining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61882b9",
   "metadata": {},
   "source": [
    "This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.\n",
    "\n",
    "Fine-tuning, on the other hand, is the training done after a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. Wait — why not simply train directly for the final task? There are a couple of reasons:\n",
    "\n",
    "The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).\n",
    "Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "For the same reason, the amount of time and resources needed to get good results are much lower.\n",
    "For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is “transferred,” hence the term transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bac6c0",
   "metadata": {},
   "source": [
    "Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.\n",
    "\n",
    "This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51f5b8",
   "metadata": {},
   "source": [
    "### Transformers - General Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a2f57",
   "metadata": {},
   "source": [
    "![transformers-3](images/transformers_blocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93900bd",
   "metadata": {},
   "source": [
    "The model is primarily composed of two blocks:\n",
    "\n",
    "- Encoder (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "\n",
    "- Decoder (right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
    "\n",
    "Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "- Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "- Decoder-only models: Good for generative tasks such as text generation.\n",
    "- Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a049310",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac733f6e",
   "metadata": {},
   "source": [
    "A key feature of Transformer models is that they are built with special layers called attention layers. In fact, the title of the paper introducing the Transformer architecture was “Attention Is All You Need”! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n",
    "\n",
    "To put this into context, consider the task of translating text from English to French. Given the input “You like this course”, a translation model will need to also attend to the adjacent word “You” to get the proper translation for the word “like”, because in French the verb “like” is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating “this” the model will also need to pay attention to the word “course”, because “this” translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of “this”. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.\n",
    "\n",
    "The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ab81d",
   "metadata": {},
   "source": [
    "### Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92021129",
   "metadata": {},
   "source": [
    "The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
    "\n",
    "To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b8044",
   "metadata": {},
   "source": [
    "![transformers-3](images/transformers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464d1e9",
   "metadata": {},
   "source": [
    "Note that the the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919f76a",
   "metadata": {},
   "source": [
    "The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7896d8",
   "metadata": {},
   "source": [
    "### Encoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25090254",
   "metadata": {},
   "source": [
    "Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.\n",
    "\n",
    "The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
    "\n",
    "Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- ALBERT\n",
    "- BERT\n",
    "- DistilBERT\n",
    "- ELECTRA\n",
    "- RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254133ab",
   "metadata": {},
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00457fe",
   "metadata": {},
   "source": [
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.\n",
    "\n",
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n",
    "These models are best suited for tasks involving text generation.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- CTRL\n",
    "- GPT\n",
    "- GPT-2\n",
    "- Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa80d48",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebe6bd",
   "metadata": {},
   "source": [
    "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.\n",
    "\n",
    "The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, T5 is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.\n",
    "\n",
    "Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- BART\n",
    "- mBART\n",
    "- Marian\n",
    "- T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a12da",
   "metadata": {},
   "source": [
    "### Bias and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ae3b3",
   "metadata": {},
   "source": [
    "https://huggingface.co/course/chapter1/8?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da3716c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f37476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
